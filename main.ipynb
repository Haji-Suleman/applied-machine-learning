{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"housing.csv\")\n",
    "df = df.dropna()\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned = pd.get_dummies(df_cleaned, columns=[\"ocean_proximity\"], dtype=int)\n",
    "df_cleaned[\"total_bedrooms\"] = df_cleaned[\"total_bedrooms\"].fillna(\n",
    "    df_cleaned[\"total_bedrooms\"].median()\n",
    ")\n",
    "\n",
    "X = df_cleaned[\n",
    "    [\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"housing_median_age\",\n",
    "        \"total_rooms\",\n",
    "        \"total_bedrooms\",\n",
    "        \"population\",\n",
    "        \"households\",\n",
    "        \"median_house_value\",\n",
    "        \"ocean_proximity_<1H OCEAN\",\n",
    "        \"ocean_proximity_INLAND\",\n",
    "        \"ocean_proximity_NEAR BAY\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"median_income\"]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(y.shape, X.shape)\n",
    "\n",
    "X = torch.tensor(np.array(X_scaled)).type(dtype=torch.float32)\n",
    "y = torch.tensor(np.array(y)).type(dtype=torch.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "class Housing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Layer1 = nn.Linear(in_features=11, out_features=13)\n",
    "        self.Layer2 = nn.Linear(in_features=13, out_features=20)\n",
    "        self.Layer3 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.Layer3(self.ReLU(self.Layer2(self.ReLU(self.Layer1(X)))))\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model24 = Housing()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model24.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model24.train()\n",
    "    y_preds = model24(X_train).squeeze()\n",
    "    loss = loss_fn(y_preds, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model24.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_test_preds = model24(X_test).squeeze()\n",
    "        test_loss = loss_fn(y_test_preds, y_test)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Training Loss: {loss.item()} | Testing Loss: {test_loss.item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haji Suleman\\AppData\\Local\\Temp\\ipykernel_6424\\1332255194.py:60: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y = torch.tensor(np.array(y), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 1.0470306873321533 | Testing Loss: 0.85666424036026\n",
      "Epoch: 20 | Training Loss: 0.9907930493354797 | Testing Loss: 0.80319744348526\n",
      "Epoch: 40 | Training Loss: 0.9292519092559814 | Testing Loss: 0.7436577081680298\n",
      "Epoch: 60 | Training Loss: 0.8458612561225891 | Testing Loss: 0.6658174395561218\n",
      "Epoch: 80 | Training Loss: 0.7423845529556274 | Testing Loss: 0.5733669996261597\n",
      "Epoch: 100 | Training Loss: 0.6363466382026672 | Testing Loss: 0.48168012499809265\n",
      "Epoch: 120 | Training Loss: 0.5407956838607788 | Testing Loss: 0.4026115834712982\n",
      "Epoch: 140 | Training Loss: 0.465734601020813 | Testing Loss: 0.344007283449173\n",
      "Epoch: 160 | Training Loss: 0.41112014651298523 | Testing Loss: 0.3053702712059021\n",
      "Epoch: 180 | Training Loss: 0.3743809461593628 | Testing Loss: 0.2821577489376068\n",
      "Epoch: 200 | Training Loss: 0.34981459379196167 | Testing Loss: 0.2677740156650543\n",
      "Epoch: 220 | Training Loss: 0.33183255791664124 | Testing Loss: 0.25736936926841736\n",
      "Epoch: 240 | Training Loss: 0.3183095455169678 | Testing Loss: 0.24931232631206512\n",
      "Epoch: 260 | Training Loss: 0.3075236678123474 | Testing Loss: 0.24263061583042145\n",
      "Epoch: 280 | Training Loss: 0.2985388934612274 | Testing Loss: 0.23663203418254852\n",
      "Epoch: 300 | Training Loss: 0.29076671600341797 | Testing Loss: 0.23168762028217316\n",
      "Epoch: 320 | Training Loss: 0.28388097882270813 | Testing Loss: 0.22718000411987305\n",
      "Epoch: 340 | Training Loss: 0.2781103551387787 | Testing Loss: 0.22333727777004242\n",
      "Epoch: 360 | Training Loss: 0.27307891845703125 | Testing Loss: 0.22045470774173737\n",
      "Epoch: 380 | Training Loss: 0.2682785391807556 | Testing Loss: 0.21783947944641113\n",
      "Epoch: 400 | Training Loss: 0.26354843378067017 | Testing Loss: 0.21516765654087067\n",
      "Epoch: 420 | Training Loss: 0.25907382369041443 | Testing Loss: 0.21255303919315338\n",
      "Epoch: 440 | Training Loss: 0.25453439354896545 | Testing Loss: 0.21024850010871887\n",
      "Epoch: 460 | Training Loss: 0.24960045516490936 | Testing Loss: 0.20786292850971222\n",
      "Epoch: 480 | Training Loss: 0.2440730780363083 | Testing Loss: 0.20561379194259644\n",
      "Epoch: 500 | Training Loss: 0.23862075805664062 | Testing Loss: 0.20333589613437653\n",
      "Epoch: 520 | Training Loss: 0.23325452208518982 | Testing Loss: 0.2010532170534134\n",
      "Epoch: 540 | Training Loss: 0.22765149176120758 | Testing Loss: 0.1988150179386139\n",
      "Epoch: 560 | Training Loss: 0.22124536335468292 | Testing Loss: 0.1972949206829071\n",
      "Epoch: 580 | Training Loss: 0.21453304588794708 | Testing Loss: 0.19541475176811218\n",
      "Epoch: 600 | Training Loss: 0.20763099193572998 | Testing Loss: 0.19337864220142365\n",
      "Epoch: 620 | Training Loss: 0.2007327526807785 | Testing Loss: 0.19101861119270325\n",
      "Epoch: 640 | Training Loss: 0.19419099390506744 | Testing Loss: 0.1886391043663025\n",
      "Epoch: 660 | Training Loss: 0.18794354796409607 | Testing Loss: 0.1870153397321701\n",
      "Epoch: 680 | Training Loss: 0.1822495311498642 | Testing Loss: 0.18565036356449127\n",
      "Epoch: 700 | Training Loss: 0.17698396742343903 | Testing Loss: 0.18383164703845978\n",
      "Epoch: 720 | Training Loss: 0.17220497131347656 | Testing Loss: 0.18215668201446533\n",
      "Epoch: 740 | Training Loss: 0.1679101586341858 | Testing Loss: 0.18080009520053864\n",
      "Epoch: 760 | Training Loss: 0.1641186624765396 | Testing Loss: 0.17963910102844238\n",
      "Epoch: 780 | Training Loss: 0.16081185638904572 | Testing Loss: 0.1780046969652176\n",
      "Epoch: 800 | Training Loss: 0.15783077478408813 | Testing Loss: 0.17641369998455048\n",
      "Epoch: 820 | Training Loss: 0.15523706376552582 | Testing Loss: 0.17523051798343658\n",
      "Epoch: 840 | Training Loss: 0.15290467441082 | Testing Loss: 0.17407941818237305\n",
      "Epoch: 860 | Training Loss: 0.1508997082710266 | Testing Loss: 0.17309899628162384\n",
      "Epoch: 880 | Training Loss: 0.1490088403224945 | Testing Loss: 0.17239469289779663\n",
      "Epoch: 900 | Training Loss: 0.1472627818584442 | Testing Loss: 0.17215031385421753\n",
      "Epoch: 920 | Training Loss: 0.1456417292356491 | Testing Loss: 0.17164868116378784\n",
      "Epoch: 940 | Training Loss: 0.14409641921520233 | Testing Loss: 0.17104260623455048\n",
      "Epoch: 960 | Training Loss: 0.14267931878566742 | Testing Loss: 0.17097841203212738\n",
      "Epoch: 980 | Training Loss: 0.14128457009792328 | Testing Loss: 0.17061592638492584\n",
      "Epoch: 1000 | Training Loss: 0.13989906013011932 | Testing Loss: 0.1702822744846344\n",
      "Epoch: 1020 | Training Loss: 0.1386050432920456 | Testing Loss: 0.1697281450033188\n",
      "Epoch: 1040 | Training Loss: 0.13737347722053528 | Testing Loss: 0.16955949366092682\n",
      "Epoch: 1060 | Training Loss: 0.13604988157749176 | Testing Loss: 0.16893234848976135\n",
      "Epoch: 1080 | Training Loss: 0.1347680687904358 | Testing Loss: 0.16834066808223724\n",
      "Epoch: 1100 | Training Loss: 0.13358838856220245 | Testing Loss: 0.16847100853919983\n",
      "Epoch: 1120 | Training Loss: 0.1322891265153885 | Testing Loss: 0.16798213124275208\n",
      "Epoch: 1140 | Training Loss: 0.13111385703086853 | Testing Loss: 0.1681004762649536\n",
      "Epoch: 1160 | Training Loss: 0.12997207045555115 | Testing Loss: 0.1682348996400833\n",
      "Epoch: 1180 | Training Loss: 0.12881185114383698 | Testing Loss: 0.1684577614068985\n",
      "Epoch: 1200 | Training Loss: 0.1276371330022812 | Testing Loss: 0.16885963082313538\n",
      "Epoch: 1220 | Training Loss: 0.12641474604606628 | Testing Loss: 0.16906364262104034\n",
      "Epoch: 1240 | Training Loss: 0.12526974081993103 | Testing Loss: 0.17007872462272644\n",
      "Epoch: 1260 | Training Loss: 0.12405196577310562 | Testing Loss: 0.1702011078596115\n",
      "Epoch: 1280 | Training Loss: 0.12284370511770248 | Testing Loss: 0.16976843774318695\n",
      "Epoch: 1300 | Training Loss: 0.12167225778102875 | Testing Loss: 0.1696104258298874\n",
      "Epoch: 1320 | Training Loss: 0.1206265240907669 | Testing Loss: 0.1700705885887146\n",
      "Epoch: 1340 | Training Loss: 0.11966808140277863 | Testing Loss: 0.17071636021137238\n",
      "Epoch: 1360 | Training Loss: 0.11870865523815155 | Testing Loss: 0.17162147164344788\n",
      "Epoch: 1380 | Training Loss: 0.11785762012004852 | Testing Loss: 0.17256662249565125\n",
      "Epoch: 1400 | Training Loss: 0.117080457508564 | Testing Loss: 0.1732378602027893\n",
      "Epoch: 1420 | Training Loss: 0.11635419726371765 | Testing Loss: 0.1741369366645813\n",
      "Epoch: 1440 | Training Loss: 0.11563071608543396 | Testing Loss: 0.17464306950569153\n",
      "Epoch: 1460 | Training Loss: 0.11489707231521606 | Testing Loss: 0.17522577941417694\n",
      "Epoch: 1480 | Training Loss: 0.11420214921236038 | Testing Loss: 0.17612014710903168\n",
      "Epoch: 1500 | Training Loss: 0.11354298889636993 | Testing Loss: 0.17676442861557007\n",
      "Epoch: 1520 | Training Loss: 0.11289644986391068 | Testing Loss: 0.17742429673671722\n",
      "Epoch: 1540 | Training Loss: 0.1123245507478714 | Testing Loss: 0.1781008094549179\n",
      "Epoch: 1560 | Training Loss: 0.11182360351085663 | Testing Loss: 0.17840199172496796\n",
      "Epoch: 1580 | Training Loss: 0.11133375018835068 | Testing Loss: 0.1785719245672226\n",
      "Epoch: 1600 | Training Loss: 0.11083168536424637 | Testing Loss: 0.1787281185388565\n",
      "Epoch: 1620 | Training Loss: 0.1103287786245346 | Testing Loss: 0.17899750173091888\n",
      "Epoch: 1640 | Training Loss: 0.10969092696905136 | Testing Loss: 0.17965348064899445\n",
      "Epoch: 1660 | Training Loss: 0.10910093784332275 | Testing Loss: 0.17974533140659332\n",
      "Epoch: 1680 | Training Loss: 0.10855714976787567 | Testing Loss: 0.17990148067474365\n",
      "Epoch: 1700 | Training Loss: 0.1080494299530983 | Testing Loss: 0.18048515915870667\n",
      "Epoch: 1720 | Training Loss: 0.10757710039615631 | Testing Loss: 0.18086481094360352\n",
      "Epoch: 1740 | Training Loss: 0.10710760205984116 | Testing Loss: 0.1815638393163681\n",
      "Epoch: 1760 | Training Loss: 0.10666114091873169 | Testing Loss: 0.1823970079421997\n",
      "Epoch: 1780 | Training Loss: 0.10620437562465668 | Testing Loss: 0.18280723690986633\n",
      "Epoch: 1800 | Training Loss: 0.1057584211230278 | Testing Loss: 0.18310314416885376\n",
      "Epoch: 1820 | Training Loss: 0.10533986240625381 | Testing Loss: 0.1833493560552597\n",
      "Epoch: 1840 | Training Loss: 0.10493691265583038 | Testing Loss: 0.18365538120269775\n",
      "Epoch: 1860 | Training Loss: 0.10457773506641388 | Testing Loss: 0.18410007655620575\n",
      "Epoch: 1880 | Training Loss: 0.10421078652143478 | Testing Loss: 0.18444404006004333\n",
      "Epoch: 1900 | Training Loss: 0.10384798794984818 | Testing Loss: 0.18494828045368195\n",
      "Epoch: 1920 | Training Loss: 0.10352226346731186 | Testing Loss: 0.1851125806570053\n",
      "Epoch: 1940 | Training Loss: 0.10321865230798721 | Testing Loss: 0.1854836493730545\n",
      "Epoch: 1960 | Training Loss: 0.102927565574646 | Testing Loss: 0.18593624234199524\n",
      "Epoch: 1980 | Training Loss: 0.10265610367059708 | Testing Loss: 0.186456561088562\n",
      "Epoch: 2000 | Training Loss: 0.1023574024438858 | Testing Loss: 0.18711045384407043\n",
      "Epoch: 2020 | Training Loss: 0.10203172266483307 | Testing Loss: 0.1877359002828598\n",
      "Epoch: 2040 | Training Loss: 0.1017242893576622 | Testing Loss: 0.1882142275571823\n",
      "Epoch: 2060 | Training Loss: 0.10144148766994476 | Testing Loss: 0.18859192728996277\n",
      "Epoch: 2080 | Training Loss: 0.10118873417377472 | Testing Loss: 0.18905885517597198\n",
      "Epoch: 2100 | Training Loss: 0.10094494372606277 | Testing Loss: 0.18912367522716522\n",
      "Epoch: 2120 | Training Loss: 0.10070142149925232 | Testing Loss: 0.18927785754203796\n",
      "Epoch: 2140 | Training Loss: 0.10046160221099854 | Testing Loss: 0.18949563801288605\n",
      "Epoch: 2160 | Training Loss: 0.10021919012069702 | Testing Loss: 0.18956919014453888\n",
      "Epoch: 2180 | Training Loss: 0.09998159110546112 | Testing Loss: 0.19009017944335938\n",
      "Epoch: 2200 | Training Loss: 0.0997421145439148 | Testing Loss: 0.1903270035982132\n",
      "Epoch: 2220 | Training Loss: 0.09951289743185043 | Testing Loss: 0.19069704413414001\n",
      "Epoch: 2240 | Training Loss: 0.09928855299949646 | Testing Loss: 0.1909584403038025\n",
      "Epoch: 2260 | Training Loss: 0.0990712121129036 | Testing Loss: 0.1913107931613922\n",
      "Epoch: 2280 | Training Loss: 0.09885706752538681 | Testing Loss: 0.19139514863491058\n",
      "Epoch: 2300 | Training Loss: 0.09865526109933853 | Testing Loss: 0.1917801797389984\n",
      "Epoch: 2320 | Training Loss: 0.09845136851072311 | Testing Loss: 0.1918104588985443\n",
      "Epoch: 2340 | Training Loss: 0.09823629260063171 | Testing Loss: 0.19189560413360596\n",
      "Epoch: 2360 | Training Loss: 0.09793947637081146 | Testing Loss: 0.19201096892356873\n",
      "Epoch: 2380 | Training Loss: 0.09761547297239304 | Testing Loss: 0.19250062108039856\n",
      "Epoch: 2400 | Training Loss: 0.0973309725522995 | Testing Loss: 0.19267180562019348\n",
      "Epoch: 2420 | Training Loss: 0.09704380482435226 | Testing Loss: 0.19262896478176117\n",
      "Epoch: 2440 | Training Loss: 0.09670103341341019 | Testing Loss: 0.19254939258098602\n",
      "Epoch: 2460 | Training Loss: 0.09642771631479263 | Testing Loss: 0.19263288378715515\n",
      "Epoch: 2480 | Training Loss: 0.09621889144182205 | Testing Loss: 0.19244538247585297\n",
      "Epoch: 2500 | Training Loss: 0.09600140154361725 | Testing Loss: 0.1924877017736435\n",
      "Epoch: 2520 | Training Loss: 0.09576499462127686 | Testing Loss: 0.1923150271177292\n",
      "Epoch: 2540 | Training Loss: 0.09548667073249817 | Testing Loss: 0.19215455651283264\n",
      "Epoch: 2560 | Training Loss: 0.09525106847286224 | Testing Loss: 0.19179318845272064\n",
      "Epoch: 2580 | Training Loss: 0.09499300271272659 | Testing Loss: 0.19160453975200653\n",
      "Epoch: 2600 | Training Loss: 0.09473816305398941 | Testing Loss: 0.1913822442293167\n",
      "Epoch: 2620 | Training Loss: 0.09451547265052795 | Testing Loss: 0.1913747936487198\n",
      "Epoch: 2640 | Training Loss: 0.09426169842481613 | Testing Loss: 0.19139252603054047\n",
      "Epoch: 2660 | Training Loss: 0.09403684735298157 | Testing Loss: 0.1909831315279007\n",
      "Epoch: 2680 | Training Loss: 0.09379579871892929 | Testing Loss: 0.1908094882965088\n",
      "Epoch: 2700 | Training Loss: 0.09356512874364853 | Testing Loss: 0.1908096820116043\n",
      "Epoch: 2720 | Training Loss: 0.09336322546005249 | Testing Loss: 0.19111788272857666\n",
      "Epoch: 2740 | Training Loss: 0.09317922592163086 | Testing Loss: 0.1914074718952179\n",
      "Epoch: 2760 | Training Loss: 0.09297247231006622 | Testing Loss: 0.19198957085609436\n",
      "Epoch: 2780 | Training Loss: 0.09270119667053223 | Testing Loss: 0.19263309240341187\n",
      "Epoch: 2800 | Training Loss: 0.092414990067482 | Testing Loss: 0.1930570900440216\n",
      "Epoch: 2820 | Training Loss: 0.09220502525568008 | Testing Loss: 0.19303791224956512\n",
      "Epoch: 2840 | Training Loss: 0.09198883175849915 | Testing Loss: 0.1929694563150406\n",
      "Epoch: 2860 | Training Loss: 0.09176180511713028 | Testing Loss: 0.19322814047336578\n",
      "Epoch: 2880 | Training Loss: 0.09150881320238113 | Testing Loss: 0.1935673952102661\n",
      "Epoch: 2900 | Training Loss: 0.0911785215139389 | Testing Loss: 0.19347834587097168\n",
      "Epoch: 2920 | Training Loss: 0.09086921066045761 | Testing Loss: 0.1937272697687149\n",
      "Epoch: 2940 | Training Loss: 0.09062172472476959 | Testing Loss: 0.19401578605175018\n",
      "Epoch: 2960 | Training Loss: 0.09039343148469925 | Testing Loss: 0.19427557289600372\n",
      "Epoch: 2980 | Training Loss: 0.09018994122743607 | Testing Loss: 0.1943879872560501\n",
      "Epoch: 3000 | Training Loss: 0.09001009166240692 | Testing Loss: 0.19440819323062897\n",
      "Epoch: 3020 | Training Loss: 0.08984315395355225 | Testing Loss: 0.19450587034225464\n",
      "Epoch: 3040 | Training Loss: 0.08967620879411697 | Testing Loss: 0.1948881298303604\n",
      "Epoch: 3060 | Training Loss: 0.08953095227479935 | Testing Loss: 0.19525234401226044\n",
      "Epoch: 3080 | Training Loss: 0.08938141167163849 | Testing Loss: 0.19548127055168152\n",
      "Epoch: 3100 | Training Loss: 0.08925587683916092 | Testing Loss: 0.19593803584575653\n",
      "Epoch: 3120 | Training Loss: 0.08911751955747604 | Testing Loss: 0.19611459970474243\n",
      "Epoch: 3140 | Training Loss: 0.0889887809753418 | Testing Loss: 0.19605842232704163\n",
      "Epoch: 3160 | Training Loss: 0.08879939466714859 | Testing Loss: 0.19668331742286682\n",
      "Epoch: 3180 | Training Loss: 0.08862883597612381 | Testing Loss: 0.1970880627632141\n",
      "Epoch: 3200 | Training Loss: 0.08847379684448242 | Testing Loss: 0.19740721583366394\n",
      "Epoch: 3220 | Training Loss: 0.08832057565450668 | Testing Loss: 0.19779209792613983\n",
      "Epoch: 3240 | Training Loss: 0.08817330747842789 | Testing Loss: 0.1977766454219818\n",
      "Epoch: 3260 | Training Loss: 0.08801267296075821 | Testing Loss: 0.19795604050159454\n",
      "Epoch: 3280 | Training Loss: 0.08785879611968994 | Testing Loss: 0.197999507188797\n",
      "Epoch: 3300 | Training Loss: 0.08767420798540115 | Testing Loss: 0.19818811118602753\n",
      "Epoch: 3320 | Training Loss: 0.08751870691776276 | Testing Loss: 0.19869571924209595\n",
      "Epoch: 3340 | Training Loss: 0.08735428005456924 | Testing Loss: 0.19865228235721588\n",
      "Epoch: 3360 | Training Loss: 0.08720751106739044 | Testing Loss: 0.19889405369758606\n",
      "Epoch: 3380 | Training Loss: 0.0870739221572876 | Testing Loss: 0.19926714897155762\n",
      "Epoch: 3400 | Training Loss: 0.08693845570087433 | Testing Loss: 0.19965220987796783\n",
      "Epoch: 3420 | Training Loss: 0.0868130549788475 | Testing Loss: 0.19992485642433167\n",
      "Epoch: 3440 | Training Loss: 0.08668038249015808 | Testing Loss: 0.19992883503437042\n",
      "Epoch: 3460 | Training Loss: 0.08655115216970444 | Testing Loss: 0.1998339742422104\n",
      "Epoch: 3480 | Training Loss: 0.08643155544996262 | Testing Loss: 0.20021383464336395\n",
      "Epoch: 3500 | Training Loss: 0.08631520718336105 | Testing Loss: 0.20037899911403656\n",
      "Epoch: 3520 | Training Loss: 0.08618650585412979 | Testing Loss: 0.2004219889640808\n",
      "Epoch: 3540 | Training Loss: 0.08606450259685516 | Testing Loss: 0.20046621561050415\n",
      "Epoch: 3560 | Training Loss: 0.08593976497650146 | Testing Loss: 0.20053008198738098\n",
      "Epoch: 3580 | Training Loss: 0.08579599857330322 | Testing Loss: 0.20052871108055115\n",
      "Epoch: 3600 | Training Loss: 0.08565318584442139 | Testing Loss: 0.200408473610878\n",
      "Epoch: 3620 | Training Loss: 0.08551938831806183 | Testing Loss: 0.2005929946899414\n",
      "Epoch: 3640 | Training Loss: 0.08537416905164719 | Testing Loss: 0.20062308013439178\n",
      "Epoch: 3660 | Training Loss: 0.08524271845817566 | Testing Loss: 0.2006961852312088\n",
      "Epoch: 3680 | Training Loss: 0.08511852473020554 | Testing Loss: 0.2006980925798416\n",
      "Epoch: 3700 | Training Loss: 0.08500650525093079 | Testing Loss: 0.20072917640209198\n",
      "Epoch: 3720 | Training Loss: 0.08488421887159348 | Testing Loss: 0.20053298771381378\n",
      "Epoch: 3740 | Training Loss: 0.08475673198699951 | Testing Loss: 0.2007855325937271\n",
      "Epoch: 3760 | Training Loss: 0.08460930734872818 | Testing Loss: 0.20061752200126648\n",
      "Epoch: 3780 | Training Loss: 0.08446173369884491 | Testing Loss: 0.2008078545331955\n",
      "Epoch: 3800 | Training Loss: 0.08430925011634827 | Testing Loss: 0.20058147609233856\n",
      "Epoch: 3820 | Training Loss: 0.0841672345995903 | Testing Loss: 0.2006627917289734\n",
      "Epoch: 3840 | Training Loss: 0.08403610438108444 | Testing Loss: 0.20090807974338531\n",
      "Epoch: 3860 | Training Loss: 0.083893783390522 | Testing Loss: 0.2009028196334839\n",
      "Epoch: 3880 | Training Loss: 0.08373773097991943 | Testing Loss: 0.20098520815372467\n",
      "Epoch: 3900 | Training Loss: 0.08360034227371216 | Testing Loss: 0.2011062651872635\n",
      "Epoch: 3920 | Training Loss: 0.08346568048000336 | Testing Loss: 0.2010403871536255\n",
      "Epoch: 3940 | Training Loss: 0.08334004878997803 | Testing Loss: 0.20103654265403748\n",
      "Epoch: 3960 | Training Loss: 0.08322291076183319 | Testing Loss: 0.20116132497787476\n",
      "Epoch: 3980 | Training Loss: 0.08311339467763901 | Testing Loss: 0.20113304257392883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"housing.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# UPDATE: Handle missing values\n",
    "# -------------------------------\n",
    "df = df.dropna()  # drop rows with missing values (simple approach)\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# One-hot encoding for categorical column\n",
    "df_cleaned = pd.get_dummies(df_cleaned, columns=[\"ocean_proximity\"], dtype=int)\n",
    "\n",
    "# No need to fill total_bedrooms after dropna, but keeping safety\n",
    "df_cleaned[\"total_bedrooms\"] = df_cleaned[\"total_bedrooms\"].fillna(\n",
    "    df_cleaned[\"total_bedrooms\"].median()\n",
    ")\n",
    "\n",
    "# Feature selection\n",
    "X = df_cleaned[\n",
    "    [\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"housing_median_age\",\n",
    "        \"total_rooms\",\n",
    "        \"total_bedrooms\",\n",
    "        \"population\",\n",
    "        \"households\",\n",
    "        \"median_house_value\",\n",
    "        \"ocean_proximity_<1H OCEAN\",\n",
    "        \"ocean_proximity_INLAND\",\n",
    "        \"ocean_proximity_NEAR BAY\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "y = df[\"median_income\"].values.reshape(-1, 1)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y)\n",
    "\n",
    "y = torch.tensor(y_scaled, dtype=torch.float32).squeeze()\n",
    "\n",
    "# -------------------------------\n",
    "# UPDATE: Feature Scaling (Very Important)\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(np.array(X_scaled), dtype=torch.float32)\n",
    "y = torch.tensor(np.array(y), dtype=torch.float32)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# UPDATE: Neural Network with ReLU\n",
    "# -------------------------------\n",
    "class Housing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Layer1 = nn.Linear(in_features=11, out_features=13)\n",
    "        self.Layer2 = nn.Linear(in_features=13, out_features=20)\n",
    "        self.Layer3 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.ReLU = nn.ReLU()  # Non-linearity\n",
    "\n",
    "    def forward(self, X):\n",
    "        # UPDATE: Using ReLU between layers (better for learning)\n",
    "        X = self.ReLU(self.Layer1(X))\n",
    "        X = self.ReLU(self.Layer2(X))\n",
    "        return self.Layer3(X)\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model24 = Housing()\n",
    "\n",
    "# Loss function (Mean Squared Error for regression)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# -------------------------------\n",
    "# UPDATE: Lower learning rate (stability)\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(params=model24.parameters(), lr=0.0005)\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model24.train()\n",
    "\n",
    "    # Forward pass\n",
    "    y_preds = model24(X_train).squeeze()\n",
    "\n",
    "    # -------------------------------\n",
    "    # UPDATE: Correct loss order (predictions first)\n",
    "    # -------------------------------\n",
    "    loss = loss_fn(y_preds, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model24.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_test_preds = model24(X_test).squeeze()\n",
    "        test_loss = loss_fn(y_test_preds, y_test)\n",
    "\n",
    "    # Print progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | Training Loss: {loss.item()} | Testing Loss: {test_loss.item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191841be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_train_test(data, test_radio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_radio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df83b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_train_test(data=df, test_radio=0.2)\n",
    "\n",
    "print(f\"Shape of the X: {X.shape}\\n\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67174efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xFFFFFFFF < test_ratio * 2**32\n",
    "\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
